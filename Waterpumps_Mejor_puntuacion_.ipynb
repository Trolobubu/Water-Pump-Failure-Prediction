{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbfcb9f9-d9ce-451a-b144-2ab4a188d94f",
   "metadata": {},
   "source": [
    "Proyecto Waterpumps — Informe y Entrenamiento (HGB denso) - Mejor intento\n",
    "\n",
    "Objetivo: predecir el estado de cada waterpoint (functional / non functional / functional needs repair).\n",
    "Restricciones: sin APIs ni datos externos.\n",
    "Métrica: Macro-F1 (promedio de F1 por clase).\n",
    "Resumen del enfoque:\n",
    "\n",
    "Feature Engineering temporal y geográfica (incluye KMeans sobre lon/lat/altura).\n",
    "\n",
    "Preprocesado denso: numéricas con imputación+escalado; categóricas con TopN + One-Hot.\n",
    "\n",
    "Modelo: HistGradientBoostingClassifier con early stopping.\n",
    "\n",
    "Validación: holdout 80/20 estratificado.\n",
    "\n",
    "Genera submission_hgb_dense.csv."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412b7db3-dbbe-4f50-81e0-74ed15e933a1",
   "metadata": {},
   "source": [
    "0.Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf34b233-4382-49fa-a910-e3bef0649236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos encontrados:\n",
      "True -> C:\\Users\\judit\\waterpumps\\train_features.csv\n",
      "True -> C:\\Users\\judit\\waterpumps\\labels.csv\n",
      "True -> C:\\Users\\judit\\waterpumps\\test_features.csv\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 0) Setup y rutas\n",
    "# =========================\n",
    "\n",
    "# --- Setup básico /\n",
    "import os\n",
    "os.environ[\"LOKY_MAX_CPU_COUNT\"] = \"1\"  # evita loky raro en Windows\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# --- Config ---\n",
    "SEED = 42\n",
    "ID_COL = \"id\"\n",
    "TARGET_COL = \"status_group\"\n",
    "\n",
    "# Rutas \n",
    "TRAIN_CSV  = r\"C:\\Users\\judit\\waterpumps\\train_features.csv\"\n",
    "LABELS_CSV = r\"C:\\Users\\judit\\waterpumps\\labels.csv\"           # cambia si tu archivo difiere\n",
    "TEST_CSV   = r\"C:\\Users\\judit\\waterpumps\\test_features.csv\"\n",
    "\n",
    "SUBM_DIR = r\"C:\\Users\\judit\\waterpumps\\submission\"\n",
    "SUBM_OUT = os.path.join(SUBM_DIR, \"submission_hgb_dense.csv\")\n",
    "os.makedirs(SUBM_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Archivos encontrados:\")\n",
    "for p in [TRAIN_CSV, LABELS_CSV, TEST_CSV]:\n",
    "    print(os.path.exists(p), \"->\", p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64010648-d076-44fa-ad1a-ac27ee045487",
   "metadata": {},
   "source": [
    "1) Carga, unión con etiquetas y revisión rápida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81c90010-6ab3-4bfc-b5ab-1e6de94ded1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (59400, 40) | Test: (14850, 40)\n",
      "Distribución de clases (train): {'functional': 0.543, 'non functional': 0.384, 'functional needs repair': 0.073}\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 1) CARGA Y MERGE\n",
    "# =========================\n",
    "# Carga\n",
    "trainX = pd.read_csv(TRAIN_CSV)\n",
    "labels = pd.read_csv(LABELS_CSV)\n",
    "testX  = pd.read_csv(TEST_CSV)\n",
    "\n",
    "# Dejar solo columnas necesarias en labels\n",
    "\n",
    "labels = labels[[c for c in labels.columns if c in {ID_COL, TARGET_COL}]]\n",
    "\n",
    "# Merge train + etiquetas\n",
    "\n",
    "df = trainX.merge(labels, on=ID_COL, how=\"inner\")\n",
    "X = df.drop(columns=[TARGET_COL])\n",
    "y = df[TARGET_COL]\n",
    "\n",
    "print(f\"Train: {X.shape} | Test: {testX.shape}\")\n",
    "print(\"Distribución de clases (train):\", y.value_counts(normalize=True).round(3).to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aeeb442-aa16-4a68-90ba-58768f11ad05",
   "metadata": {},
   "source": [
    "2) Feature Engineering (temporal, edad, geográfico, logs/ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef2e61ed-455f-4165-98e2-285d369f49ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 2) FEATURE ENGINEERING\n",
    "# =========================\n",
    "def add_engineered_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "\n",
    "    # --- Temporal desde date_recorded ---\n",
    "    dr = pd.to_datetime(out.get(\"date_recorded\"), errors=\"coerce\")\n",
    "    out[\"rec_year\"] = dr.dt.year\n",
    "    out[\"rec_month\"] = dr.dt.month\n",
    "    out[\"rec_dayofweek\"] = dr.dt.dayofweek\n",
    "\n",
    "    # --- Edad del pozo y bins ---\n",
    "    cy = out.get(\"construction_year\", pd.Series(index=out.index)).replace({0: np.nan})\n",
    "    out[\"well_age\"] = out[\"rec_year\"] - cy\n",
    "    out.loc[out[\"well_age\"] < 0, \"well_age\"] = np.nan\n",
    "    out[\"well_age_bin\"] = pd.cut(\n",
    "        out[\"well_age\"],\n",
    "        bins=[-1, 0, 5, 10, 20, 40, 70, 120],\n",
    "        labels=[\"unknown\",\"0-5\",\"5-10\",\"10-20\",\"20-40\",\"40-70\",\"70+\"]\n",
    "    )\n",
    "\n",
    "    # --- Geo mínimos y flags ---\n",
    "    for col in [\"longitude\", \"latitude\", \"gps_height\"]:\n",
    "        if col not in out.columns:\n",
    "            out[col] = np.nan\n",
    "\n",
    "    out[\"gps_is_zero\"] = (\n",
    "        (out[\"longitude\"].fillna(0)==0) |\n",
    "        (out[\"latitude\"].fillna(0)==0) |\n",
    "        (out[\"gps_height\"].fillna(0)==0)\n",
    "    ).astype(int)\n",
    "\n",
    "    # --- Logs y ratio ---\n",
    "    for c in [\"amount_tsh\",\"population\"]:\n",
    "        if c not in out.columns:\n",
    "            out[c] = np.nan\n",
    "    out[\"log_population\"] = np.log1p(out[\"population\"].clip(lower=0))\n",
    "    out[\"log_amount_tsh\"] = np.log1p(out[\"amount_tsh\"].clip(lower=0))\n",
    "    out[\"tsh_per_capita\"] = out[\"amount_tsh\"] / (out[\"population\"].fillna(0) + 1)\n",
    "\n",
    "    # --- Bins geo gruesos ---\n",
    "    out[\"lat_bin\"] = (out[\"latitude\"] * 10).round(0)\n",
    "    out[\"lon_bin\"] = (out[\"longitude\"] * 10).round(0)\n",
    "\n",
    "    out[\"height_is_zero\"] = (out[\"gps_height\"].fillna(0) == 0).astype(int)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91654f7b-a7ff-4912-a5f7-4fb5afd15ecf",
   "metadata": {},
   "source": [
    "3) KMeans geográfico (train+test) para patrones regionales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b2e290a-59c8-423d-82d7-d6b3e4af48b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenamos solo columnas geo para KMeans\n",
    "_geo_concat = pd.concat([\n",
    "    X[[\"longitude\",\"latitude\",\"gps_height\"]],\n",
    "    testX[[\"longitude\",\"latitude\",\"gps_height\"]]\n",
    "], axis=0, ignore_index=True)\n",
    "\n",
    "# Imputación simple para KMeans\n",
    "_geo = _geo_concat.copy()\n",
    "for c in [\"longitude\",\"latitude\",\"gps_height\"]:\n",
    "    _geo[c] = _geo[c].fillna(_geo[c].median())\n",
    "\n",
    "K = 35  # 25–40 suele ir bien\n",
    "kmeans = KMeans(n_clusters=K, random_state=SEED, n_init=10)\n",
    "_geo_labels = kmeans.fit_predict(_geo)\n",
    "\n",
    "# Escribimos etiqueta de cluster en train y test\n",
    "X = X.copy(); testX = testX.copy()\n",
    "X[\"_geo_cluster\"] = _geo_labels[:len(X)]\n",
    "testX[\"_geo_cluster\"] = _geo_labels[len(X):]\n",
    "\n",
    "# Aplicamos FE principal\n",
    "X_fe = add_engineered_features(X)\n",
    "test_fe = add_engineered_features(testX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481715d2-8e4e-483a-8943-4381eca874e0",
   "metadata": {},
   "source": [
    "4) Inferencia de columnas (num/cat) y limpieza de ruido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7a80610-0142-4fb7-bfd2-080b02a1cc1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numéricas: 21 | Cats_low: 22 | Cats_high: 5\n"
     ]
    }
   ],
   "source": [
    "concat = pd.concat([\n",
    "    X_fe.drop(columns=[ID_COL], errors=\"ignore\"),\n",
    "    test_fe.drop(columns=[ID_COL], errors=\"ignore\")\n",
    "], axis=0, ignore_index=True)\n",
    "\n",
    "num_cols_all = [c for c in concat.columns if pd.api.types.is_numeric_dtype(concat[c])]\n",
    "cat_cols_all = [c for c in concat.columns if not pd.api.types.is_numeric_dtype(concat[c])]\n",
    "\n",
    "# Excluir texto libre muy ruidoso\n",
    "drop_high_noise = {\"wpt_name\",\"scheme_name\",\"recorded_by\",\"subvillage\"}\n",
    "cat_cols_all = [c for c in cat_cols_all if c not in drop_high_noise]\n",
    "\n",
    "# Cardinalidad\n",
    "card = concat[cat_cols_all].nunique(dropna=True)\n",
    "low_cat_cols  = [c for c in cat_cols_all if card[c] <= 50]\n",
    "high_cat_cols = [c for c in cat_cols_all if card[c] >  50]\n",
    "\n",
    "num_cols = [c for c in num_cols_all if c != ID_COL]\n",
    "\n",
    "print(f\"Numéricas: {len(num_cols)} | Cats_low: {len(low_cat_cols)} | Cats_high: {len(high_cat_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc3703a-f3c7-40b4-9baa-fa19e3fadcbe",
   "metadata": {},
   "source": [
    "5) Reductor TopN para categóricas de alta cardinalidad + Preprocesador denso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb349e5b-39cb-4076-a11f-6ce3b22cfa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopNCategoryReducer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Mantiene las Top-N categorías por frecuencia en cada columna; el resto -> 'other'.\n",
    "    Devuelve DataFrame para convivir bien con OneHotEncoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, top_n=40, min_count=30):\n",
    "        self.top_n = top_n\n",
    "        self.min_count = min_count\n",
    "        self.keepers_ = {}\n",
    "        self.columns_ = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = pd.DataFrame(X).copy()\n",
    "        self.columns_ = list(X.columns)\n",
    "        for c in self.columns_:\n",
    "            counts = X[c].astype(\"object\").value_counts(dropna=False)\n",
    "            keep = counts[counts >= self.min_count].index[: self.top_n]\n",
    "            self.keepers_[c] = set(map(lambda z: \"NA\" if pd.isna(z) else str(z), keep))\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = pd.DataFrame(X, columns=self.columns_).copy()\n",
    "        for c in self.columns_:\n",
    "            vals = X[c].astype(\"object\")\n",
    "            X[c] = vals.where(vals.isin(self.keepers_[c]), other=\"other\")\n",
    "            X[c] = X[c].fillna(\"NA\").astype(str)\n",
    "        return X\n",
    "\n",
    "# Pipelines por tipo\n",
    "num_pipe = Pipeline(steps=[\n",
    "    (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scale\", StandardScaler())  # denso\n",
    "])\n",
    "\n",
    "low_cat_pipe = Pipeline(steps=[\n",
    "    (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "])\n",
    "\n",
    "high_cat_pipe = Pipeline(steps=[\n",
    "    (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"topn\", TopNCategoryReducer(top_n=40, min_count=30)),\n",
    "    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "])\n",
    "\n",
    "transformers = []\n",
    "if num_cols:      transformers.append((\"num\",     num_pipe,      num_cols))\n",
    "if low_cat_cols:  transformers.append((\"lowcat\",  low_cat_pipe,  low_cat_cols))\n",
    "if high_cat_cols: transformers.append((\"highcat\", high_cat_pipe, high_cat_cols))\n",
    "\n",
    "pre_dense = ColumnTransformer(transformers=transformers, remainder=\"drop\", sparse_threshold=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c024b6-2fff-47d7-becd-8c0d2e4ade67",
   "metadata": {},
   "source": [
    "6) Modelo: HistGradientBoostingClassifier (con early stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "673848f2-0ead-499a-af1d-72ac4e09b1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_hgb = HistGradientBoostingClassifier(\n",
    "    learning_rate=0.12,\n",
    "    max_depth=12,\n",
    "    max_iter=400,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1,\n",
    "    n_iter_no_change=30,\n",
    "    random_state=SEED,\n",
    "    l2_regularization=1e-3,\n",
    "    max_bins=255\n",
    ")\n",
    "\n",
    "model = Pipeline(steps=[\n",
    "    (\"pre\", pre_dense),\n",
    "    (\"clf\", clf_hgb)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22912caa-c7e0-4c55-9499-945e14aa26c9",
   "metadata": {},
   "source": [
    "7) Validación holdout (80/20 estratificado) y pesos de clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc462b32-b0b9-4eb1-9072-5f48f2715cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-F1 (holdout): 0.6560\n",
      "\n",
      "Classification report (holdout):\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "             functional     0.8338    0.7316    0.7793      6452\n",
      "functional needs repair     0.2836    0.7022    0.4040       863\n",
      "         non functional     0.8310    0.7430    0.7845      4565\n",
      "\n",
      "               accuracy                         0.7338     11880\n",
      "              macro avg     0.6494    0.7256    0.6560     11880\n",
      "           weighted avg     0.7927    0.7338    0.7541     11880\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>functional</th>\n",
       "      <th>functional needs repair</th>\n",
       "      <th>non functional</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>functional</th>\n",
       "      <td>4720</td>\n",
       "      <td>1123</td>\n",
       "      <td>609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>functional needs repair</th>\n",
       "      <td>176</td>\n",
       "      <td>606</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>non functional</th>\n",
       "      <td>765</td>\n",
       "      <td>408</td>\n",
       "      <td>3392</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         functional  functional needs repair  non functional\n",
       "functional                     4720                     1123             609\n",
       "functional needs repair         176                      606              81\n",
       "non functional                  765                      408            3392"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr, X_va, y_tr, y_va = train_test_split(\n",
    "    X_fe, y, test_size=0.2, stratify=y, random_state=SEED\n",
    ")\n",
    "\n",
    "classes_ = np.unique(y_tr)\n",
    "cw = compute_class_weight(class_weight=\"balanced\", classes=classes_, y=y_tr)\n",
    "cw_map = {cls: w for cls, w in zip(classes_, cw)}\n",
    "sw_tr = y_tr.map(cw_map).values\n",
    "\n",
    "# Fit con early stopping (usa validation_fraction interno)\n",
    "model.fit(X_tr, y_tr, clf__sample_weight=sw_tr)\n",
    "\n",
    "pred_va = model.predict(X_va)\n",
    "macro_f1 = f1_score(y_va, pred_va, average=\"macro\")\n",
    "print(f\"Macro-F1 (holdout): {macro_f1:.4f}\")\n",
    "\n",
    "print(\"\\nClassification report (holdout):\")\n",
    "print(classification_report(y_va, pred_va, digits=4))\n",
    "\n",
    "labels_sorted = sorted(y.unique())\n",
    "cm = confusion_matrix(y_va, pred_va, labels=labels_sorted)\n",
    "pd.DataFrame(cm, index=labels_sorted, columns=labels_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37beb490-97e1-4f48-b208-7bc5e387d9e2",
   "metadata": {},
   "source": [
    "8) Entrenamiento final en todo el train y generación de submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2686490-519a-4a11-ad0e-eab816f33615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Submission guardada en: C:\\Users\\judit\\waterpumps\\submission\\submission_hgb_dense.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>status_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50785</td>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51630</td>\n",
       "      <td>functional needs repair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17168</td>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45559</td>\n",
       "      <td>non functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49871</td>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id             status_group\n",
       "0  50785               functional\n",
       "1  51630  functional needs repair\n",
       "2  17168               functional\n",
       "3  45559           non functional\n",
       "4  49871               functional"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recalcular sample_weight en todo el train\n",
    "classes_all = np.unique(y)\n",
    "cw_all = compute_class_weight(class_weight=\"balanced\", classes=classes_all, y=y)\n",
    "cw_map_all = {cls: w for cls, w in zip(classes_all, cw_all)}\n",
    "sw_all = y.map(cw_map_all).values\n",
    "\n",
    "# Fit final\n",
    "model.fit(X_fe, y, clf__sample_weight=sw_all)\n",
    "\n",
    "# Predicción sobre test y guardado de submission\n",
    "preds = model.predict(test_fe)\n",
    "submission = pd.DataFrame({ID_COL: testX[ID_COL], TARGET_COL: preds})\n",
    "submission.to_csv(SUBM_OUT, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"✅ Submission guardada en:\", SUBM_OUT)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbc8c04-2bc9-4288-a990-cf7f7fe42b16",
   "metadata": {},
   "source": [
    "9) Notas para el informe \n",
    "\n",
    "Métrica: Macro-F1, apropiada con clases desbalanceadas.\n",
    "\n",
    "Desbalanceo: class_weight='balanced' + sample_weight por frecuencia.\n",
    "\n",
    "FE: variables temporales, edad del pozo, flags GPS, bins geográficos, log1p y ratio tsh_per_capita, y cluster KMeans geo (K≈35).\n",
    "\n",
    "Preprocesado: numéricas (mediana + escalado), categóricas baja cardinalidad (OHE), alta (TopN→OHE). Exclusión de texto libre muy ruidoso (wpt_name, scheme_name, recorded_by, subvillage).\n",
    "\n",
    "Modelo: HGB con early stopping; funciona bien en denso y captura no linealidades.\n",
    "\n",
    "Reproducibilidad: rutas fijas a C:\\Users\\judit\\waterpumps\\... y random_state global."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
